{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have input data $X = (X_1, X_2, ..., X_n)$ and corresponding class labels $Y = (Y_1, Y_2, ..., Y_n)$ where $n$ represents the number of observations/instances (i.e., unique samples). Much of statistical learning concerns trying to model this relationship between our data's $X$ and $Y$. In particular, we assert that the $Y$'s were produced/generated by some underlying function $f(X)$, and that there is inevitably some noise and systematic, implicit bias and error $\\epsilon$ that cannot be captured by any $f(X)$. Thus, we have:\n",
    "\n",
    "$Y = f(X) + \\epsilon$\n",
    "\n",
    "Statistical learning concerns either **prediction** or **inference**:\n",
    "\n",
    "**Prediction:** concerns trying to learn a function $\\hat{f}(X)$ that is as close as possible to the true function $f(X)$. This allows us to estimate $Y$ values for any new input data $X$.\n",
    "\n",
    "**Inference:** concerns trying to understand/model the _relationship_ between $X$ and $Y$, effectively learning how the data was generated.\n",
    "\n",
    "Independent of this, if you have access to gold truth labels $Y$, and you make use of them for your modelling, then you are working on a **supervised** learning task. If you do not have or make use of $Y$ values, and you are only concerned with the input data $X$, you are working on an **unsupervised** learning task.\n",
    "\n",
    "Methods that are centered around modeling and prediction of a quantitative response variable (ex, number of taxi pickups, number of bike rentals, etc) are called **regressions** (and Ridge, LASSO, etc).\n",
    "\n",
    "When the response variable is categorical, then the problem is no longer called a regression problem but is instead labeled as a **classification** problem. The goal is to attempt to classify each observation into a category (aka, class or cluster) defined by Y, based on a set of predictor variables X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression uses the logistic function to model the probability of some categorical, typically binary response using the covariate values. \n",
    "\n",
    "$P(Y=1) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}} = \\frac{1}{1+ e^{-(\\beta_0+\\beta_1X)}}$\n",
    "\n",
    "Logistic regression derives $\\beta$ values using maximum likelihood estimation, using Fischer scoring iterations to converge to estimated values. This is because there is often no closed form solution to the derivative of the loss function. In cases of perfect separation, the beta value will continuously increase, as this simultaneously increases the log-likelihood of data in class 1 being assigned to class 1 and data in class 0 being assigned to class 0. Therefore the loss function would find it optimal to continuously increase beta, and you would never converge to a finite solution.\n",
    "\n",
    "We're looking for the coefficients which minimize the loss function: $L(\\beta_0,\\beta_1)= - \\Sigma[y_ilogp_i+(1-y_i)log(1-p_i)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![affine](images/affine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see from above that we are summing up the likelihoods of each individual observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutations & Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In permutations, order matters. In combinations, order does not matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have 7 marbles in a bag. You pick one and then replace it. How many permutations exist for 4 picks? Well there are 7 options for the first “pick” and 7 options for the second. So there are $7^4 = 2,401$ outcomes for 4 picks.\n",
    "2. You have 7 marbles in a bag. You pick one but do not replace it. How many permutations exist for 4 picks? Well there are 7 options for the first “pick” and 6 options for the second. So there are $7*6*5*4 = \\frac{7!}{(7-4)!} = 840$ outcomes for 4 picks.\n",
    "3. You have 7 marbles in a bag. How many unique combinations of 4 marbles can you make? ${n \\choose k} = 35$ combinations.\n",
    "4. You have 7 marbles in a bag. How many unique combinations of 2 can be made with replacement? In this case, since we have replacement, there can be repeating marbles in each combination. This one is the trickiest. We call the result “Bose-Einstein”. ${n + k -1\\choose k} = {8\\choose 2} = 28$ combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Combinations](images/combinations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Side Notes:**\n",
    "\n",
    "How many ways can you permute a list of n things into k spots? Well with replacement $n^k$ ways, without replacement it's $n*(n-1)*\\dots*(n-k+1)$. Note that if k = n, then these values become $n^n$ and $n!$. That is, there are $n!$ permutations of a list with n elements.\n",
    "\n",
    "A set with `n` elements has $2^n$ subsets including 0 and the full set itself (when $k = n$): $1 + \\Sigma_{k=0}^n{n\\choose k}$.\n",
    "\n",
    "Also, ${n\\choose k} = {n \\choose (n-k)}$\n",
    "\n",
    "If you have 10 empty slots on a board, and you have 4 objects to fill those slots, there are ${10 \\choose 4}$ unique combinations of placements for those objects on the board. You can think of those objects as indicators. And the placements indicating that that particular slot is selected in the unique comination out of the 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deck of Cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(PAIR) = \\frac{{13 \\choose1}{4\\choose2}{12\\choose3}{4\\choose1}{4\\choose1}{4\\choose1}}{52\\choose5}= \\frac{1,098,240}{2,598,960}$\n",
    "\n",
    "$P(2-PAIR) = \\frac{{13 \\choose2}{4\\choose2}{4\\choose2}{11\\choose1}{4\\choose1}}{52\\choose5}= \\frac{123,552}{2,598,960}$\n",
    "\n",
    "$P(TRIPLE) = \\frac{{13 \\choose1}{4\\choose3}{12\\choose2}{4\\choose1}{4\\choose1}}{52\\choose5}= \\frac{54,912}{2,598,960}$\n",
    "\n",
    "$P(FULL|HOUSE) = \\frac{{13 \\choose1}{4\\choose3}{12\\choose1}{4\\choose2}}{52\\choose5} = \\frac{3,744}{2,598,960}$\n",
    "\n",
    "$P(4-OFA-KIND) = \\frac{{13 \\choose1}{4\\choose4}{12\\choose1}{4\\choose1}}{52\\choose5}= \\frac{624}{2,598,960}$\n",
    "\n",
    "$P(STRAIGHT) = \\frac{10{4\\choose1}{4\\choose1}{4\\choose1}{4\\choose1}{4\\choose1}-36-4}{52\\choose5} = \\frac{10,200}{2,598,960}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You decide to roll a die twice. Is the probability that the sum of the two rolls equals 12 higher, lower, or the same as the probability that the sum equals 11?**\n",
    "- P(Sum = 12) = P(Roll 1 = 6 ∩ Roll 2 = 6) = 1/36\n",
    "- P(Sum = 11) = P(Roll 1 = 6 ∩ Roll 2 = 5) + P(Roll 1 = 5 ∩ Roll 2 = 6) = 2/36\n",
    "\n",
    "**6 basketball players are playing 3-on-3. How many possible matchups are there?**\n",
    "- An intuitive approach is to say “I’m choosing 3 of 6 players to be on team A, so ${6 \\choose 3}$.” However, picking players 1, 2, and 3 for team A is the same as picking players 4, 5, and 6. Therefore, we have to divide by two to get $\\frac{1}{2}{6 \\choose 3} = 10$\n",
    "\n",
    "**A standard deck of 52 cards is well shuffled and put in a stack. Find the probability that the ace of spades is directly behind the ace of diamonds or that ace of spades is first and the ace of diamonds is last.**\n",
    "- Think of arranging the cards in a circle, such that the first card is “behind” the last card. 51 cards could be behind the ace of diamonds, and each is equally likely. Therefore the probability is simply 1/51.\n",
    "\n",
    "**10 Americans are surveyed. One in every 100 is named Rachel. Find the probability that at least one of the sampled Americans is named Rachel (given the size of the country, you can treat samples as independent).**\n",
    "- The probability that the first sample isn’t a Rachel is 0.99. Since samples are independent, the probability that no Rachels are sampled is $(0.99)^10$. The probability of at least one Rachel is then 1 − 0.9910.\n",
    "\n",
    "**How many ways can you permute the letters in the word PEPPER? ALABAMA? LALALAAA?**\n",
    "- ${6\\choose3}{3\\choose2} = 60$ (Because looking for combinations of the 3 letter \"P\"s in 6 spots, then the 2 letter \"E\"s from remaining 3 spots. Last letter pre-determined)\n",
    "- ${7\\choose4}{3\\choose1}{2\\choose1} = 210$ (Same as above)\n",
    "- ${8\\choose5} = {8\\choose3} = 56$ (Because placement of the As determines placement of the Ls\n",
    "\n",
    "**Birthday Problem: There are 23 people in a room. What is the probability that two or more people in the group share the same birthday?**\n",
    "- $1 - \\frac{365*364* \\dots * (343)}{365^{23}} = 1 - 0.4995 = 50.1\\%$\n",
    "\n",
    "**In a room of `n` people, what is the expected number of distinct birthdays?**\n",
    "- Let $I_j$ be the indicator that someone is born on day j: $E(I_j) = 1 - (\\frac{364}{365})^n$ so by linearity we have $E(X) = 365(1 - (\\frac{364}{365}))^n$. So in a room of `365` people, there are expected to be $365(1-(\\frac{364}{365})^{365}) = 231$ unique birthdays.\n",
    "\n",
    "**What's the expected number of birthday matches in a room of `n` people?\n",
    "- Well the probability that any one pair is a match is $\\frac{1}{365}$ and there are $n \\choose 2$ total possible pairs, so by linearity the expected number of matches is $\\frac{n \\choose 2}{365}$. So in a room of 365 people, there are $\\frac{365*182}{365} = 182 expected matches$.\n",
    "\n",
    "**If you roll 3 standard dice, find the probability that at least two show the same number**\n",
    "- $1 - \\frac{5*4}{6*6} = \\frac{4}{9}$\n",
    "\n",
    "**Suppose an urn contains w white balls and b black balls. Of the balls in the urn, each is equally likely\n",
    "to be chosen. In n draws with replacement, what is the probability you pull exactly x white balls?**\n",
    "- $P(X=x) = {n\\choose x}(\\frac{w}{w+b})^x(\\frac{b}{w+b})^{n-x}$\n",
    "\n",
    "\n",
    "**During WW2, the allies captured 25 German tanks, each with a unique serial number. Assume the Germans have t tanks total, which we are trying to estimate, which have numbers 1, 2, $\\dots$, t. Assume each tank is equally likely to be captured. The highest serial number captured is 115. U.S. spies estimate there are 1400 tanks total while American statisticians estimate there are 256. Calculate the probability of seeing a maximum of 115 for t = 1400 and t = 256. Which seems more realistic?**\n",
    "- If t = 1400, there are $1400 \\choose 25$ ways to choose the 25 tanks. Then, there are $115−1 \\choose 25−1$ ways to get a maximum of 115: one of the tanks must be 115, and the other 24 must be below 115. The probability of seeing a maximum of M = 115 given t = 1400 is then (# of ways 115 can be the max out of 25 tanks / # of ways you can have 25 tanks in general) $P(M = 115|t = 1400) = \\frac{115−1 \\choose 25−1}{1400 \\choose 25} \\approx 1.18 · 10^{−29}$.\n",
    "- If t turns out to be 256, there are instead $256 \\choose 25$ ways to choose the 25 tanks. The number of ways to observe a maximum of 115 does not change. The probability of seeing a maximum of M = 115 is then $P(M = 115|t = 256) = \\frac{115−1 \\choose 25−1}{256 \\choose 25} \\approx 8.94 · 10^{−11}$. So the statisticians’ guess appears more likely. Fun fact: the true number of tanks turned out to be 255!\n",
    "\n",
    "**On a dating site, users can select 5 out of 24 adjectives to describe themselves. A match is declared between two users if they match on at least 4 adjectives. If Alice and Bob randomly pick adjectives, what is the probability that they form a match?**\n",
    "- $\\frac{{5\\choose4}{19\\choose1}+{5\\choose5}{19\\choose0}}{{24\\choose5}}=\\frac{4}{1771} \\approx 0.2\\%$\n",
    "\n",
    "**How many ways 12 persons may be divided into three groups of 4 persons each?**\n",
    "- $\\frac{{12\\choose4}{8\\choose4}{4\\choose4}}{3!} = 5775$\n",
    "\n",
    "**Expected number of flips to get 1 Heads? 2 consecutive heads? 2 consecutive heads?**\n",
    "- 2\n",
    "- 6\n",
    "- 14\n",
    "\n",
    "\n",
    "In complicated probability questions, always try to start with constructing the naive definition (# of successful outcomes / # of total outcomes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(A|B) = \\frac{P(A\\cap B)}{P(B)}$ Alternatively: $P(A\\cap B) = P(A|B)P(B)$\n",
    "\n",
    "If $P(A\\cap B) = P(A)P(B)$ then A and B are independent. Alternatively: $P(A|B) = P(A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "- $P(A|B,C) = \\frac{P(B|A,C)P(A|C)}{P(B|C)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOTP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $P(A|B) = \\frac{P(B|A) P(A)}{P(B|A)P(A) + P(B|A^c)P(A^c)}$\n",
    "- $P(A|C) = \\Sigma_{i = 1}^nP(A|B_i,C)P(B_i|C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inclusion-Exclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $P(A ∪ B) = P(A) + P(B) − P(A ∩ B)$  \n",
    "- $P(A ∪ B ∪ C) = P(A) + P(B) + P(C) − P(A ∩ B) − P(B ∩ C) − P(A ∩ C) + P(A ∩ B ∩ C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A father of three mentions that he’s going to pick up his daughters (plural). Given he has at least two daughters, find the probability that all three of his kids are girls. 99 Daughters?**\n",
    "- $P(G_3 | \\geq G_2) = \\frac{P(\\geq G_2|G_3)P(G_3)}{P(\\geq G_2)} = \\frac{1*\\frac{1}{8}}{{3\\choose2}P(G_2) + {3\\choose3}P(G_3)} = \\frac{\\frac{1}{8}}{\\frac{1}{2}} = \\frac{1}{4}$\n",
    "- $P(G_{100} | \\geq G_{99}) = \\frac{P(\\geq G_{99}|G_{100})P(G_{100})}{P(\\geq G_{99})} = \\frac{1*\\frac{1}{2^{100}}}{{100\\choose99}P(G_{99}) + {100\\choose100}P(G_{100})} = \\frac{\\frac{1}{2^{100}}}{100\\frac{1}{2^{100}}+1\\frac{1}{2^{100}}} = \\frac{1}{101}$\n",
    "\n",
    "**A rare disease called Onlygirlitis affects 1 in 100 pregnant women, preventing them from giving birth to boys. Find the probability that Jane has Onlygirlitis given she has three children, all of whom are girls.**\n",
    "- $P(O|G_3) = \\frac{P(G_3|O)P(O)}{P(G_3)} = \\frac{1*\\frac{1}{100}}{P(G_3|O)P(O)+P(G_3|O^c)P(O^c)} = \\frac{\\frac{1}{100}}{1\\frac{1}{100}+\\frac{1}{8}\\frac{99}{100}} = \\frac{1}{13.375} \\approx 0.075$\n",
    "\n",
    "**Jane is pregnant. Find the probability that Jane’s fourth child is a girl given her first three are girls.**\n",
    "- $P(4^G|G_3) = P(4^G|G_3,O)P(O|G_3)+P(4^G|G_3,O^c)P(O^c|G_3) = 1(0.075)+\\frac{1}{2} (0.925) = 0.5375$\n",
    "\n",
    "**30% of people who use conditioner experience split ends. 20% of people who use conditioner shower at the MAC. 60% of those using conditioner and showering at the MAC have split ends. Your roommate uses conditioner, and has split-ends. Find the probability she’s been showering at the MAC.**\n",
    "- $P(M|C,S) = \\frac{P(C|M,S)*P(M|S)}{P(C|S)} = \\frac{0.6*0.2}{0.3} = 40\\%$\n",
    "\n",
    "**All these people in Seattle tell you it’s raining there**\n",
    "- Conditional?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X $\\sim N(\\mu, \\sigma^2)$ \n",
    "- Continuous\n",
    "- $E(X) = \\mu$\n",
    "- $Var(X) = \\sigma^2$\n",
    "- $P(X=x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $X \\sim Pois(\\lambda)$\n",
    "- Discrete\n",
    "- $E(X) = \\lambda$\n",
    "- $Var(X) = \\lambda$\n",
    "- $P(X = k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}$\n",
    "- So at an intersection, say that on average there are 2 accidents per month. A reasonable way to model this would be to say $X \\sim Pois(2)$. The number of accidents in two months? $X \\sim Pois(4)$\n",
    "- If $X \\sim Pois(\\lambda_1)$ and $Y \\sim Pois(\\lambda_2)$ then $X + Y \\sim Pois(\\lambda_1 + \\lambda_2)$\n",
    "- In a Poisson Process, there is an exponential distribution between every occurance in a poisson distribution. So in our explanaitions above we have $X \\sim Pois(2)$: expected number of accidents per month is 2. So our Exponential distribution between accidents is $X \\sim Expo(2)$, which means expected time before next accident is $\\frac{1}{2}$ months. Which makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernouli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $X \\sim Bern(p)$\n",
    "- Discrete\n",
    "- $E(X) = p$\n",
    "- $Var(X) = pq$\n",
    "- $P(X = 1) = p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $X \\sim Bin(n, p)$\n",
    "- Discrete\n",
    "- $E(X) = np$\n",
    "- $Var(X) = npq$\n",
    "- $P(X = k) = {n \\choose k}p^k(1-p)^{n-k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $X \\sim Geom(p)$\n",
    "- $E(X) = \\frac{(1-p)}{p}$\n",
    "- $Var(X) = \\frac{(1-p)}{p^2}$\n",
    "- $P(X = k) = p(1-p)^k$\n",
    "- This models the expected number of failures before success where success has probability `p`.\n",
    "- So if you're rolling a dice, and success is a `6` then you're expected to have $E(X) = \\frac{\\frac{5}{6}}{\\frac{1}{6}} = 5$ failures before you get a `6`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $X \\sim Expo(\\lambda)$\n",
    "- Continuous, Memoryless property\n",
    "- $E(X) = \\frac{1}{\\lambda}$\n",
    "- $Var(X) = \\frac{1}{\\lambda^2}$\n",
    "- $P(X = x) = \\lambda e^{-\\lambda x}$\n",
    "- So if you have $\\lambda = \\frac{1}{10}$ then your expected value is 10. So if it's time until next phone call, you're expected to wait 10 minutes. But if no call has happened after 5 minutes, you're still expected to wait 10 minutes because of the memoryless property. Given the CDF, the probability of getting a call within the first 5 minutes is $1 - e^{-\\lambda x} = 1 - e^{-\\frac{1}{2}} = 0.39$\n",
    "- In a Poisson Process, there is an exponential distribution between every occurance in a poisson distribution. So in our explanaitions above we have $X \\sim Pois(2)$: expected number of accidents per month is 2. So our Exponential distribution between accidents is $X \\sim Expo(2)$, which means expected time before next accident is $\\frac{1}{2}$ months = 15 days. Which makes sense.\n",
    "- The minimum of iid Exponential random variables is $ min(X_1 + X_2 + \\dots + X_k) \\sim Expo(\\lambda_1 + \\lambda_2 + \\dots + \\lambda_k)$ so the Expected value of the minimum is $\\frac{1}{\\lambda_1 + \\lambda_2 + \\dots + \\lambda_k}$. So say we had 5 intersections, each with an accident occuring $X\\sim Pois(2)$ per month. Then the distribution of the first accident at any one of the 5 intersections is $Y \\sim Expo(2+2+2+2+2 = 10)$, the expected value is$\\frac{1}{10}$, which means the first accident is expected to happen after only 3 days ($\\frac{1}{10}$month).\n",
    "- Now the maximum of iid Exponential random variables is $ max(X_1 + X_2 + \\dots + X_k) \\sim Expo(\\lambda_1) + Expo(2\\lambda_2) + \\dots + Expo(k\\lambda_k)$ so the Expected value of the maximum is $\\frac{1}{\\lambda_1} + \\frac{1}{2\\lambda_2} + \\dots + \\frac{1}{k\\lambda_k}$.So say we had 5 intersections, each with an accident occuring $X\\sim Pois(2)$ per month. Then the distribution of the 10th accident at those 5 intersections combined is $Y \\sim Expo(\\frac{1}{2}+\\frac{1}{4}+\\frac{1}{6}+\\frac{1}{8}+\\frac{1}{10} = \\frac{137}{120} = 1.14)$, the expected value is$\\frac{1}{1.14} = 0.876$, which means the tenth accident is expected to happen after $30*0.876 = 26.3$ days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $X \\sim FS(p)$\n",
    "- $E(X) = \\frac{1}{p}$\n",
    "- $Var(X) = \\frac{(1-p)}{p^2}$\n",
    "- $P(X = k) = p(1-p)^k$\n",
    "- This models the expected number of trials to achieve first success where success has probability `p`.\n",
    "- This is exactly the same as the geometric distribution except that it includes the successful trial.\n",
    "- So if you're rolling a dice, and success is a `6` then you're expected to roll the dice $E(X) = \\frac{1}{\\frac{1}{6}} = 6$ times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $X \\sim U(a,b)$\n",
    "- $E(X) = \\frac{a+b}{2}$\n",
    "- $Var(X) = \\frac{(b-a)^2}{12}$\n",
    "- $P(X = k) = \\frac{1}{b-a}$\n",
    "- The $jth$ order statistic of iid $U_1, \\dots, U_n \\sim U(0,1)$ is $U_j \\sim Beta(j, n - j + 1)$. So for the maximum draw in 4 pulls, we have $X \\sim Beta(4, 1)$. The expected value of this distribution is $ \\frac{4}{4+1} = 0.8$. So the max of 4 draws from the Uniform is expected to be 0.8. The trend is basically just $\\frac{n}{n+1}$\n",
    "- For the minimum it's basically just $\\frac{1}{n+1}$ so we have min of 4 draws is expected to be $\\frac{1}{4+1} = 0.20$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Binomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $X \\sim NB(r,p)$\n",
    "- $E(X) = \\frac{r(1-p)}{p}$\n",
    "- $Var(X) = \\frac{r(1-p)}{p^2}$\n",
    "- $P(X = ?) = {r+n-1 \\choose r-1} p^r(1-p)^n$\n",
    "- Trying to find teh number of failures before rth success\n",
    "- Can be thought of ass iid Geometric distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $W \\sim HGeom(w, b, n)$\n",
    "- $E(W) = \\frac{nw}{w+b}$\n",
    "- $Var(W) = (\\frac{w+b-n}{w+b-1})(\\frac{nw}{w+b})(1-\\frac{\\frac{nw}{w+b}}{n})$\n",
    "- $P(W=k) = \\frac{{w\\choose k}{b\\choose n-k}}{w+b \\choose n}$\n",
    "- Story: suppose you are drawing n balls without replacement from an urn with w white balls and b black balls. The number of white balls you draw follows the hypergeometric distribution with parameters w, b, and n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connections between distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $X \\sim Bin(n, p), Y \\sim Bin(m, p)$ and X is independent of Y, then $X + Y \\sim Bin(n + m, p)$.  \n",
    "- In a Poisson Process, there is an exponential distribution between every occurance in a poisson distribution. So in our explanaitions above we have $X \\sim Pois(2)$: expected number of accidents per month is 2. So our Exponential distribution between accidents is $X \\sim Expo(2)$, which means expected time before next accident is $\\frac{1}{2}$ months. Which makes sense.\n",
    "- If a poisson has a large $\\mu$ then it will be approximately normally distributed. Variance equals the mean for poisson so as $\\lambda$ increases, so doe sthe variance of the distribution. The normal distribution has two parameters, obviously.\n",
    "- If $X \\sim Bin(n, p)$ and if n is large and/or p is close to $\\frac{1}{2}$, then X is approximately $\\sim N(np, np(1-p))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 12 Monday-Wednesday classes and 10 Tuesday-Thurdsay classes. You pick 4 classes at random. Find the probability of picking only one Tuesday-Thursday class.**\n",
    "- $P(X = 3) = \\frac{{12\\choose 3}{10\\choose 1}}{22 \\choose 4} = \\frac{220*10}{7315} = \\frac{2200}{7315} = 30\\%$\n",
    "\n",
    "**A textbook has `n` typos which are randomly scattereed amongst its n pages, independently. You pick a random page. What's the probability that it has no typos?**\n",
    "- There is a $(1-\\frac{1}{n})$ probability that any specific typo isn't on a given page. So that means we have $(1-\\frac{1}{n})^n$ probability that there are no typos on your page. So for 100 pages which have 100 typos, the probability that page number 73 has 0 typos is $(\\frac{99}{100})^{100} \\approx 36\\%$\n",
    "\n",
    "**What is the distribution of the maximum of 4 draws from $X_i \\sim U(0,1)$? The max of 5? 9? 99?**\n",
    "- We know that the $jth$ order statistic of iid $U_1, \\dots, U_n \\sim U(0,1)$ is $U_j \\sim Beta(j, n - j + 1)$. So for the maximum draw in 4 pulls, we have $X \\sim Beta(4, 1)$. The expected value of this distribution is $ \\frac{4}{4+1} = 0.8$. So the max of 4 draws from the Uniform is expected to be 0.8.\n",
    "- Max of 5 is expected to be $\\frac{5}{5+1} = 0.83$\n",
    "- Max of 9 is expected to be $\\frac{9}{10} = 0.9$\n",
    "- Max of 99 is expected to be $\\frac{99}{100} = 0.99$\n",
    "- So the trend is basically just $\\frac{n}{n+1}$\n",
    "\n",
    "**How about the minimum?**\n",
    "- Well in this scenario it's basically just $\\frac{1}{n+1}$\n",
    "- Min of 4 draws is expected to be $\\frac{1}{4+1} = 0.20$\n",
    "- Min of 5 is expected to be $\\frac{1}{5+1} = 0.167$\n",
    "- Min of 9 is expected to be $\\frac{1}{10} = 0.1$\n",
    "- Min of 99 is expected to be $\\frac{1}{100} = 0.01$\n",
    "\n",
    "**There are `n` people at a party. Each hangs up a coat at the check. Each leaves with a random coat. What's the expected number of people who leave with the right coat?**\n",
    "- Well the odds any given person takes the right coat is $\\frac{1}{n}$ so by linearity the number of correct coats taken $ = n\\frac{1}{n} = 1$.\n",
    "\n",
    "**How many draws from uniform distribution $\\sim U(0,1)$ before expected to be over 0.8**\n",
    "- Well the probability of \"success\" in this scenario is 0.2. So we essentially have a First Success problem. The expected number of trials to get a success is $\\frac{1}{0.2} = 5$.\n",
    "\n",
    "**There are 6 toys given out independently with Happy Meals at McDonalds. What is the expected number of Happy meals you have to buy to be expected to have at least one of each toy?**\n",
    "- Well on the first order we will get one toy. This leaves 5 left. The probability that the second toy is different from the first is $\\frac{5}{6}$. So the number of trials to be successful is $\\sim FS(\\frac{5}{6})$ so the expected values $(\\mu) = \\frac{6}{5}$ trials for you to be successful. After the second success we have a $\\frac{4}{6}$ chance of getting a new toy, and so on average $\\frac{6}{4}$ trials to be successful. Ultimately we're expected to draw $1 + \\frac{6}{5} + \\frac{6}{4} +\\frac{6}{3} +\\frac{6}{2} +\\frac{6}{1} = 1 + 1\\frac{1}{5} + 1\\frac{1}{2} + 2 + 3 + 6 = 14\\frac{7}{10}$ happy meals.\n",
    "\n",
    "**You call 3 Ubers and 2 Lyfts. Their arrival times are all iid. What's the probability that the first 3 cars are all Ubers?**\n",
    "- So we can do number of successful permutations divided by number of total permutations $= \\frac{3!2!}{5!} = \\frac{1}{10}$\n",
    "\n",
    "**What's the probability that the minimum of 10 iid samples $\\sim U(0,1)$ is $>0.2$? The maximum?**\n",
    "- This is the same as asking what the probability that each individual draw is $> 0.2$. So we have $(0.8)^{10} = 10.7\\%$\n",
    "- So here aren't looking for the probability that EVERY observation is above some threshold, just that at least one is. So we're looking for 1 - the probabiblity that they are all below 0.2: $1 - (0.2)^{10} = 1- 0.0000001024 \\approx 1$\n",
    "\n",
    "**What's the probability that the minimum of 10 iid samples $\\sim U(0,1)$ is $<0.2$? The maximum?**\n",
    "- So here we have $1 - (0.8)^{10} = 89.3\\%$\n",
    "- And $(0.2)^{10} = .00001024\\%$\n",
    "\n",
    "**What's the expected number of cards you have to draw from a shuffled deck before you pick the first Ace (not counting the Ace)?\n",
    "- $48*\\frac{1}{5} = 9.6$ by linearity\n",
    "\n",
    "\n",
    "**You select a number X randomly from the uniform distribution $\\sim(0,1)$. Then you repeatedly, and independently, draw number 𝑌1,𝑌2,.... from the uniform distribution $\\sim(0,1)$ until you get a number larger than 𝑋/2, then you stop. The expected number of draws that Mr. B makes equals (a) 2ln2 (b) ln 2 (c) 2/e (d) 6/e**\n",
    "- Something to do with Geometric Disstribution $(1 - \\frac{X}{2})$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In an A/B test, you are testing your original page / experience (A) against a single variation (B) to see which will result in a higher conversion rate. Variation B might feature a multitude of changes (i.e. a ‘cluster’) of changes, or an isolated change.\n",
    "- In an A/B/n test, you are testing more than two variations of a page at once. “N” refers to the number of versions being tested, anywhere from two versions to the “nth” version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You test each individual change, isolated one against another, by mixing and matching every possible combination available.\n",
    "\n",
    "Imagine you want to test a homepage redesign with four changes in a single variation:\n",
    "- Change A: New hero banner\n",
    "- Change B: New call-to-action (CTA) copy\n",
    "- Change C: New CTA color\n",
    "- Change D: New value proposition statement\n",
    "\n",
    "![Multivariate](images/multivariate.png)\n",
    "\n",
    "Hypothetically, let’s assume that each change has the following impact on your conversion rate:\n",
    "- Change A = +10%\n",
    "- Change B = +5%\n",
    "- Change C = -25%\n",
    "- Change D = +5%\n",
    "\n",
    "If you were to run a classic A/B test―your current control page (A) versus a combination of all four changes at once (B)―you would get a hypothetical decrease of -5% overall (10% + 5% – 25% +5%). You would assume that your re-design did not work and most likely discard the ideas.\n",
    "\n",
    "With a multivariate test, however, each would be a variation. Multivariate testing is great because it shows you the positive or negative impact of every single change, and every single combination of every change, resulting in the most ideal combination (in this theoretical example: A + B + D). However, this strategy is kind of impossible in the real world. Even if you have a ton of traffic, it would still take more time than most marketers have for a test with 15 variations to reach any kind of statistical significance.\n",
    "\n",
    "In between multivariate and a classic A/B test is a factorial design:\n",
    "- You have to be able to cluster tests, but then separate them.\n",
    "- https://www.widerfunnel.com/better-ab-testing-marketing/\n",
    "\n",
    "First, let’s imagine that our control page has a baseline conversion rate of 10% and that each variation receives 1,000 unique visitors during your test.\n",
    "\n",
    "![Multivariate](images/second.png)\n",
    "\n",
    "Given the above information, you would estimate that change A is worth a 10% lift by comparing the 11% conversion rate of variation A against the 10% conversion rate of your control. But, when estimating the value of change B, variation A must become your new baseline.\n",
    "\n",
    "![Multivariate](images/third.png)\n",
    "\n",
    "The estimated conversion rate lift of change B = (.5 / 11) = 4.5%. As you can see, the ‘value’ of change B is slightly different from the 5% difference shown above. Given the results of the factorial test, one could then do a single A/B test on the optimal combination vs control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B Testing Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/comparing-multi-armed-bandit-algorithms-on-marketing-use-cases-8de62a851831\n",
    "http://blog.locut.us/2011/09/22/proportionate-ab-testing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially you have 10 armed bandit gambling machines, and their distributions are all different, and you have to spend money to learn about their distributions.  \n",
    "You can apply this to A/B testing in the sense that each ad or page has some true conversion rate. And you collect your data in the form of visitors, and then compute some conversion rate from them, and then optimize based on that data (bayesian)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thompson Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One method is to assign a Beta distribution with alpha equal to the number who have converted and beta equal to the total number minus alpha, so that the expected conversion for each page is conversions over total visitors.\n",
    "- So if you have 10 pages then you have 10 beta distributions, and then you sample randomly from each one. The page with the highest draw is shown to the user, and again the alpha and beta of that page’s distribution updated.\n",
    "- One consideration is to what prior beta distribution to give the 10 pages. You could give them all Beta 1,1 distributions (uniform), or you could give them priors based on intuition or prior information, e.g. alpha = 1 and beta = 99, that would be a 1% conversion.  \n",
    "- If we had information on the standard deviation of the conversion than we could also try to model for that. If sd was 0.05% then we could use alpha = 4, beta = 396 to get a sd of 0.5% but still an expected value of 1% conversion.  \n",
    "- If the prior distribution bands are too narrow, then the distributions are too slow to optimize, increasing regret, if they are too wide, then they might lead to errors in the learning process. And shutting off of certain good pages/ads.  \n",
    "- Regret can be quantified as difference between best outcome and eventual outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper Confidence Bound Algorithm (UCB1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basically you have 5 ads, say, and you start each one off with a certain conversion rate, they are all equal. And you create a very large confidence interval for each of these ads for the true conversion rate. \n",
    "- I guess there are a few ways you can start, it doesn’t really matter, but you take one observation for each ad, and update the expected value and also the confidence interval. \n",
    "- So due to LLN the expected value will converge to true p, and confidence interval will get smaller and smaller. \n",
    "- The algorithm is essentially to continue to get observations for the variation with the highest upper bound on the confidence interval. Eventually, this bound will converge, and so you’ll then start sampling from a different variation, etc. until it too is no longer the max upper confidence bound.\n",
    "- Eventually you get stuck with the best performing ad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Epsilon Greedy experiments, the constant ε (valued between 0 and 1) is selected by the user before the experiment starts. When allocating contacts to different variants of the campaign, a randomly chosen variant is picked ε of the time. The other 1-ε of the time, the variant with highest known payoff is chosen. The higher ε is, the more this algorithm favors exploration. \n",
    "- Interestingly, while on average the algorithm allocates more contacts to the winning variant, for individual simulations the allocation result could vary wildly from the average behavior. Due to the small differences between the two variants, and the small number of contacts we have allocated initially, the losing variant does occasionally show higher conversion rate in the beginning of the experiment. \n",
    "- Since the algorithm is so “greedy”, the initially “losing” variant is not explored enough for the algorithm to gather sufficient data to learn the real conversion rate for either variant during the experiment. This also manifests in the noisiness of the conversion rate plot for individual simulations, which do not completely converge to the true mean even after 40,000 data points (contacts allocated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The UCB-1 is the right choice of Multi Armed Bandit algorithm for low base conversion rate, small effect size scenarios given its consistency and high tolerance to noise in the data.\n",
    "- With our predefined ε of 0.1, Epsilon Greedy out-performs the other algorithms in the most difficult case (low-conversion 5-variant). \n",
    "- Thompson Sampling could be a better choice in scenarios with higher baseline conversion rate or higher expected effect size, where the algorithm would be more stable.\n",
    "- As expected, in all simulations all three algorithms obtain higher overall payouts than the A/B test set up, with the greedier algorithms showing better performances. \n",
    "- In all of the other four scenarios, Epsilon Greedy obtained higher payout in the early phase of the experiment, but Thompson Sampling consistently overtakes Epsilon Greedy, getting greedier as more data points are collected (as seen in the allocation results), and obtains higher final overall payout. \n",
    "- This once again showcases how Thompson Sampling favors exploration in the face of uncertainty during the early stage of the experiment while remaining able to switch to favoring exploitation in the later phase of the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I tend to favour Thompson sampling because it is so dynamic and versatile. It’s statistically impossible to get stuck on a sub-optimal ad in the long run with this method. Where as with other methods this can take much longer (epsilon greedy). And with methods like UCB1, Thompson is better in situations of large effect size or high baseline conversion. From my understanding, though, UCB1 is better when you have low conversion or low effect size, because it is more resilient to noise.\n",
    "- I like to not be too greedy, so for low conversion rate or small difference in conversion rate I like the thompson sampling method, in general it gives more time to collect data than the epsilon greedy, also it isn’t wasting contacts on the worst outcomes like epsilon, but it isn’t overly conservative like the UCB1 algorithm. Also because the thompson rate is more based on data than epsilon, and includes 100% of the distribution opposed to UCB1.\n",
    "- For high difference I would still go with the Thompson, it’s been shown to pick up on high differences more quickly than other algorithms, minimizing regret.\n",
    "- At Logitech we did multiple A/B tests. I was the only technical person which was interesting, and I was given a lot of runway. So I had some freedom to mess up and learn, which I think was their goal. Anywho, I did eventually learn about how important it is to set appropriate priors to the thompson sampling method.\n",
    "- If you don’t run each variant for long enough then a suboptimal machine might be confused as optimal.\n",
    "- Another thing I learned would be to run ads for a minimum stretch of time, for us that was 2 days, or about 50 dollars total per ad before I started the optimization.\n",
    "- I initially did launch a learning campaign, about 10,000 dollars, I explored affinity groups, geographies, images, headlines, etc.\n",
    "- I also made insights on Google Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Asks a question, and then classifies based on answer: Root Node is the first node. Leaf Nodes are final nodes.\n",
    "- To pick the root node, you look at how well each individual variable estimates the dependent variable\n",
    "- So you see how many people fall on each side of each independent variable, and then see how many of those people displayed the dependent variable condition.\n",
    "- If any leaf nodes are 100% yes or no, then they are pure, if not then impure, so you have to measure impurity to decide which separation is best\n",
    "- Calculate Gini for each leaf node $1 - (P(Yes)^2) - (P(No)^2)$\n",
    "- Then calculate weighted gini for each variable\n",
    "- Then pick the variable with the lowest impurity value as the root node\n",
    "- Then for each proceeding internal node, calculate all the gini impurity scores\n",
    "- If node itself has lowest score, then no point in separating further. \n",
    "- If separating leads to lower score, then choose the variable which has the lowest score.\n",
    "- Proceed in a similar fashion until complete.\n",
    "- If you have numeric variables then calculate gini values for each two way split of the numeric variable\n",
    "- So first value would be split with one group being lowest weight and other being all other weights.\n",
    "- Second value would be one group having lowest or second lowest weight and other group being all other weights etc.\n",
    "- Choose the split with lowest gini impurity value\n",
    "- If ranked data then do essentially the same as for numeric data\n",
    "- If multi-category then calculate impurity score for each category as well as for each possible combination\n",
    "- Decision trees are great for the data they were built for, but not for classifying new samples\n",
    "- Basically you’re deciding which variable is most related to the response outcome, and then splitting the data, and repeating with the remaining variables on each side of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on decision trees, but they’re much more accurate for new data\n",
    "\n",
    "- Step 1: Get a bootstrap sample from the data set.\n",
    "- Step 2: Create a decision tree for the bootstrapped data, but only consider a few of the covariates to split the data. Consider a new set of covariates for each successive split. (Use the square root of the total number of covariates as the number of randomly selected covariates at each split.)\n",
    "- This is your first tree of the forest\n",
    "- Step 3: Go back to step 1 and repeat at least a few hundred times so that you have hundreds of individual trees\n",
    "- These trees will each be based on a completely different data set, since we bootstrapped before making each tree. They will also be very different in form because we randomly picked a certain number of features to include in each split of each node of each tree\n",
    "- This is your random forest\n",
    "- Step 4: To predict whether a new patient displays the dependent variable condition, you use the new person’s characteristics and run them through each of the hundreds of decision trees. You predict that the person will display the condition which garners the greater proportion of outcomes from the random forest\n",
    "- This is called bagging - bootstrapping data and then using the aggregate\n",
    "- Step 5: Use the out-of-bag dataset to test the accuracy of the random forest by testing each data row to see whether the forest correctly assigns the final value. The proportion of correctly predicted values is the accuracy percentage of that forest.\n",
    "- The out-of-bag data are those data which were not drawn in the bootstrap. You go through each observation, and run each observation through each tree in which it was not considered. So every data point through every tree it wasn’t used in. Then you get the overall accuracy of the forest.\n",
    "- Step 6: Go back to step 1, but change the number of randomly selected variables in Step 2\n",
    "- Step 7: Measure whether the accuracy of this forest is better than the one previous\n",
    "- Step 8: Go back to step 6\n",
    "- Step 9: Decide which random forest is your final one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada-Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does th learning rate relate to AdaBoost? What does a learning rate of 0.05 mean?\n",
    "What is amount of say?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Central Limit Theorum (CLT): The sample mean of iid random variables will approach a normal distribution as sample size `n` increases, regardless of the initial distribution.\n",
    "- Law of Large Numbers (LLN): This says that the sample mean of iid random variables will converge to $\\mu$ as `n` $->\\infty$. That is to say that when flipping a coin, the proportion of heads converges to 50\\% as you flip an infinite number of times.\n",
    "- Regression to the Mean: The idea that luck in some process may occur early on, and so later performance of that same metric/team/entity will be more average and thus \"regress back towards the mean\". For example, baseball players who had great rookie seasons just by chance. Or, more apt, a computer which randomly guesses on a T/F quiz and does well above average is still expected to perform at 50\\% accuracy on the next quiz.\n",
    "- Mix Shift: The concept of overall sales/ouput remaining the same but the proportion of the components of that output changing. That is, the angles in the pie chart change, but the pie stays the same size.\n",
    "- Training Dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
